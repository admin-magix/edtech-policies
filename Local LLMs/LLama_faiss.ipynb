{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "file_path='abstract_policies.json'\n",
    "with open(file_path, encoding='utf-8') as data_file:\n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "for link,number in zip(data,range(1,len(data)+1)):\n",
    "  messages.append({'url':link,'content':data[link]})\n",
    "\n",
    "\n",
    "new_json={'messages':messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(new_json, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abstract_policies_transformed.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "\n",
    "    metadata[\"url\"] = record.get(\"url\")\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path='abstract_policies_transformed.json',\n",
    "    jq_schema='.messages[]',\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "model_norm = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "text_splitter = SemanticChunker(\n",
    "   model_norm, breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(splits, model_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
    "\n",
    "model_basename = \"model\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "\"\"\"\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        use_strict = False,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "\"\"\"\n",
    "#To download from a specific branch, use the revision parameter, as in this example:\n",
    "\n",
    "# model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "#         revision=\"gptq-8bit-64g-actorder_True\",\n",
    "#         model_basename=model_basename,\n",
    "#         use_safetensors=True,\n",
    "#         trust_remote_code=True,\n",
    "#         use_strict = False,\n",
    "#         device=\"cuda:0\",\n",
    "#         use_cache=True,\n",
    "#         quantize_config=None,\n",
    "#         load_in_8bit=True)\n",
    "\n",
    "\n",
    "use_strict = False\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "        bits=4,\n",
    "        group_size=128,\n",
    "        desc_act=False\n",
    "    )\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        use_safetensors=True,\n",
    "        strict=use_strict,\n",
    "        model_basename=model_basename,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=quantize_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_documents(vector_store, query_metadata,question):\n",
    "    # Get all documents from the vector store\n",
    "    all_documents = vector_store.similarity_search(question)\n",
    "    # Filter documents based on the query metadata\n",
    "    filtered_docs = [doc for doc in all_documents if all(item in doc.metadata.items() for item in query_metadata.items())]\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get metadata filter from user input\n",
    "def get_metadata_filter(user_input):\n",
    "    lines = user_input.split('\\n')\n",
    "    query_metadata = {}\n",
    "    for line in lines:\n",
    "        if 'Policy: ' in line:\n",
    "            print(line.split(\":\"))\n",
    "            query_metadata['url'] = line.split(':')[1].strip()\n",
    "        \n",
    "    return query_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \" Policy: https://www.clarityenglish.com/privacy.php Do you provide the information about the identity and the contact details of the controllers and, where applicable, of the controller's representative?Companies which do not have their seat in the EEA should appoint a representative within the EU. .\"\n",
    "template=f'''SYSTEM:You are a highly knowledgeable assistant with a strong foundation in GDPR principles and guidelines, as established by the European Union.\n",
    "Your expertise encompasses data privacy, individual rights under GDPR, data processing requirements, and the obligations of data controllers and processors.\n",
    "\n",
    "ANSWER THE QUESTION WITH YES/NO:\n",
    "'Yes': if you assume that the policy has the requirements specified in the question\n",
    "'No':otherwise\n",
    "\n",
    "---------------------------------------------------------------\n",
    "Example:\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "'''\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    ")\n",
    "\n",
    "# print(pipe(template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "conversation_chain = RetrievalQA.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "        return_source_documents=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Function to answer yes/no question based on filtered documents\n",
    "# def answer_yes_no_question(filtered_docs, question):\n",
    "#     context = \" \".join([doc.page_content for doc in filtered_docs])[:4000]\n",
    "#     response = conversation_chain( f\"Context: {context}\\nQuestion: {question}\")\n",
    "#     return response\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def llm_chatbot(question):\n",
    "    query_metadata = get_metadata_filter(question)\n",
    "\n",
    "    # Filter documents based on the query metadata\n",
    "    filtered_documents = filter_documents(db, query_metadata,question)[:3]\n",
    "    # print(answer_yes_no_question(filtered_documents,question))\n",
    "    # context = \" \".join([doc.page_content for doc in filtered_documents])\n",
    "    context = \"\"\n",
    "    batch_size=5\n",
    "    for i in range(0, len(filtered_documents), batch_size):\n",
    "        batch = filtered_documents[i:i+batch_size]\n",
    "        batch_context = \" \".join([doc.page_content for doc in batch])\n",
    "        context += batch_context\n",
    "        if len(context) > 1000:  # Limit context size to prevent memory issues\n",
    "            break\n",
    "    # llm_response=conversation_chain(f\"Context: {context}\\nQuestion: {question}\")\n",
    "    # print(llm_response['result'].split(\"\\n\")[-1].split(\": \")[1])\n",
    "    # print('\\n\\nSources:')\n",
    "    # for source in llm_response[\"source_documents\"]:\n",
    "    #     print(source.page_content)\n",
    "    print(llm(f\"Context: {context}\\nQuestion: {question}\",use_gup=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "llm_chatbot(\"\"\"Policy: https://www.clarityenglish.com/privacy.php, https://www.iris.co.uk/privacy-policy/, https://www.edukey.co.uk/privacy/ \n",
    "            Do you provide the information about the identity and the contact details of the controllers and, where applicable, of the controller's representative?Companies which do not have their seat in the EEA should appoint a representative within the EU. .\n",
    "\"\"\")\n",
    "\n",
    "end = datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"search_data_simple_GDPR.txt\", \"r\")\n",
    "text = f.read()\n",
    "text = text.split(\"\\n\")\n",
    "gdprs=[]\n",
    "for line in text:\n",
    "    if not line.startswith(\"GDPR\"):\n",
    "        gdprs.append(line.replace(\"\\t\",''))\n",
    "\n",
    "gdprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = ['https://www.clarityenglish.com/privacy.php', 'https://www.iris.co.uk/privacy-policy/',' https://www.edukey.co.uk/privacy/' ]\n",
    "times=[]\n",
    "for policy in policies:\n",
    "    for gdpr in gdprs:\n",
    "        start = datetime.now()\n",
    "\n",
    "        llm_chatbot(f\"\"\"Policy: {policy}\n",
    "           {gdpr}\"\"\")\n",
    "        end = datetime.now()\n",
    "        times.append(end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(times).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "717.63/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='abstract_policies.json'\n",
    "with open(file_path, encoding='utf-8') as data_file:\n",
    "    data = json.load(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.keys())[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = list(data.keys())[30:40]\n",
    "times=[]\n",
    "for policy in policies:\n",
    "    for gdpr in gdprs[:10]:\n",
    "        start = datetime.now()\n",
    "\n",
    "        llm_chatbot(f\"\"\"Policy: {policy}\n",
    "           {gdpr}\"\"\")\n",
    "        end = datetime.now()\n",
    "        times.append(end-start)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(times).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "536.480860/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = list(data.keys())[68:118]\n",
    "times=[]\n",
    "for policy in policies:\n",
    "    for gdpr in gdprs:\n",
    "        start = datetime.now()\n",
    "\n",
    "        llm_chatbot(f\"\"\"Policy: {policy}\n",
    "           {gdpr}\"\"\")\n",
    "        end = datetime.now()\n",
    "        times.append(end-start)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
