{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory, ConversationSummaryBufferMemory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema import message_to_dict\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import threading\n",
    "import json\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "            ChatPromptTemplate,\n",
    "            SystemMessagePromptTemplate,\n",
    "            AIMessagePromptTemplate,\n",
    "            HumanMessagePromptTemplate,\n",
    "            MessagesPlaceholder\n",
    "        )\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "import re\n",
    "import openai\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, TextLoader\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key =  ##prem key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('search_data_simple_GDPR.txt', 'r') as file:\n",
    "    codes_questions = file.readlines()\n",
    "\n",
    "codes_questions\n",
    "questions = [item.strip('\\n\\t') for item in codes_questions]\n",
    "questions\n",
    "questions_all = {}\n",
    " \n",
    "for quest in range(0, len(questions), 2):\n",
    "    code = questions[quest].strip(':')\n",
    "    question = questions[quest+1]\n",
    "    questions_all[code] = question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abstract_policies.json', 'r', encoding='utf=8') as file:\n",
    "    abstracts_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input):\n",
    "    return openai_client.embeddings.create(model='text-embedding-ada-002', input=input).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings(api_key = ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for source, content in abstracts_json.items():\n",
    "    source_info = {\"source\": source}\n",
    "    documents.append(Document(metadata=source_info, page_content=content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policies = {}\n",
    "\n",
    "for key, value in list(abstracts_json.items()):\n",
    "    test_policies[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_for_answering = {}\n",
    "\n",
    "for key, value in list(questions_all.items()):\n",
    "    questions_for_answering[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.collection_exists(collection_name=\"collection_openai\"):\n",
    "    client.create_collection(collection_name=\"collection_openai\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = [get_embedding(doc.page_content) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (doc, embedding) in enumerate(zip(docs, doc_embeddings)):\n",
    "    client.upsert(\n",
    "        collection_name=\"collection_openai\",\n",
    "        points=[{\n",
    "            \"id\": index,\n",
    "            \"vector\": embedding,\n",
    "            \"payload\": {\"document_chunk\": doc.page_content, \"metadata\": doc.metadata}\n",
    "        }]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_with_embeddings = {}\n",
    "\n",
    "for questionid, question in questions_for_answering.items():\n",
    "    questions_with_embeddings[questionid] = {\"Text\": question, \"Embedding\": get_embedding(question)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_all = {}\n",
    "for link, content in test_policies.items():\n",
    "    policy_response = {}\n",
    "    for questionid, question_info in questions_with_embeddings.items():\n",
    "        question = question_info[\"Text\"]\n",
    "        question_embedding = question_info[\"Embedding\"]\n",
    "        search_result = client.search(\n",
    "            collection_name=\"collection_openai\",\n",
    "            query_vector=question_embedding,\n",
    "            query_filter={\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"key\": \"metadata.source\",\n",
    "                        \"match\": {\n",
    "                            \"value\": link\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            limit=1\n",
    "        )\n",
    "        closest_chunk = search_result[0].payload['document_chunk']\n",
    "        context = \"\"\"For the following policy:\"\"\" + f\"{closest_chunk}\" + \"\"\" answer the following question with a \"Yes\"/\"No\" answer and if the answer is \"Yes\" then provide an extract\n",
    "        from the policy that is LONGER than 200 characters and best fits the answer, otherwise return an empty string, nothing else that differs from this. Make sure that your response\n",
    "        is only in a JSON format like this and DO NOT PROVIDE ANY ADDITIONAL TEXT: `{\"Answer\": \"Your answer\", \"Extract\": \"Extract from the policy\"}`, where \" Your Answer\" \n",
    "        represents your answer to the question, \"Extract from the policy\" is the best fit extract (make sure it is composed of whole sentences and also don't include any quatiotion marks). Here is the question: \"\"\" f\"{question}\"\n",
    "\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo-0125\",\n",
    "                    response_format={ \"type\": \"json_object\" },\n",
    "                    messages=[{\"role\":\"user\",\"content\": context}]\n",
    "                )\n",
    "        policy_response[questionid] = response.choices[0].message.content\n",
    "    responses_all[link] = policy_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"\", 'w', encoding=\"utf-8\") as json_file: #save responses to a json\n",
    "    json.dump(responses_all, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
