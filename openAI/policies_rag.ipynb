{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elena\\articles_gpt\\env\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "c:\\Users\\elena\\articles_gpt\\env\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory, ConversationSummaryBufferMemory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema import message_to_dict\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import threading\n",
    "import json\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "            ChatPromptTemplate,\n",
    "            SystemMessagePromptTemplate,\n",
    "            AIMessagePromptTemplate,\n",
    "            HumanMessagePromptTemplate,\n",
    "            MessagesPlaceholder\n",
    "        )\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "import re\n",
    "import openai\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, TextLoader\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key =  ##prem key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('search_data_simple_GDPR.txt', 'r') as file:\n",
    "    codes_questions = file.readlines()\n",
    "\n",
    "codes_questions\n",
    "questions = [item.strip('\\n\\t') for item in codes_questions]\n",
    "questions\n",
    "questions_all = {}\n",
    " \n",
    "for quest in range(0, len(questions), 2):\n",
    "    code = questions[quest].strip(':')\n",
    "    question = questions[quest+1]\n",
    "    questions_all[code] = question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma()\n",
    "client = chromadb.PersistentClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input):\n",
    "    return openai_client.embeddings.create(model='text-embedding-ada-002', input=input).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=15000, chunk_overlap=1000, max_chunk_size=21000):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    all_sentences = []  \n",
    "    all_chunk_lists = []\n",
    "    overlap = ''\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > chunk_size and len(current_chunk) + len(sentence) < max_chunk_size:\n",
    "            if len(chunks) > 0:\n",
    "                overlap = ' '.join(all_chunk_lists[-1][-3:])\n",
    "            all_chunk_lists.append(all_sentences)\n",
    "            all_sentences = []\n",
    "            current_chunk = overlap + current_chunk\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = ''\n",
    "            overlap = ''\n",
    "        current_chunk += sentence + ' '\n",
    "        all_sentences.append(sentence)\n",
    "    if current_chunk and len(current_chunk) < max_chunk_size:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abstract_policies.json', 'r', encoding='utf=8') as file:\n",
    "    abstracts_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_chroma(parts, policyid, collection_chroma):\n",
    "\n",
    "    for index, chunk in enumerate(parts):\n",
    "        collection_chroma.add(\n",
    "            embeddings=[get_embedding(chunk)],\n",
    "            documents=[chunk],\n",
    "            metadatas=[{\"source\": str(policyid)}],\n",
    "            ids=[str(index)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_closing_quote(string):\n",
    "    single_quotes_count = string.count(\"'\")\n",
    "    if single_quotes_count % 2 != 0:\n",
    "        string += \"'\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_matching_braces(string):\n",
    "    stack = []\n",
    "    unmatched_indices = []\n",
    "\n",
    "    for i, char in enumerate(string):\n",
    "        if char == '{':\n",
    "            stack.append(i)\n",
    "        elif char == '}':\n",
    "            if not stack:\n",
    "                unmatched_indices.append(i)\n",
    "            else:\n",
    "                stack.pop()\n",
    "\n",
    "    for index in reversed(unmatched_indices):\n",
    "        string = string[:index] + '}' + string[index:]\n",
    "\n",
    "    while stack:\n",
    "        string += '}'\n",
    "        stack.pop()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    transformed_data = {}\n",
    "    if response:\n",
    "        for key, value in response.items():\n",
    "            value = add_matching_braces(value)\n",
    "            print(value)\n",
    "            transformed_data[key] = json.loads(value)\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_responses(policyid, policy, embedded_questions, collection_chroma):\n",
    "    policy_chunks = chunk_text(policy)\n",
    "    add_data_to_chroma(policy_chunks, policyid, collection_chroma)\n",
    "    policy_response = {}\n",
    "    for questionid, question_content in embedded_questions.items():\n",
    "        closest_chunk = collection_chroma.query(\n",
    "        query_embeddings=question_content['Embedding'],\n",
    "        n_results=1\n",
    "        )['documents'][0][0]\n",
    "        \n",
    "        context = \"\"\"For the following policy:\"\"\" + f\"{closest_chunk}\" + \"\"\" answer the following question with a \"Yes\"/\"No\" answer and if the answer is \"Yes\" then provide an extract\n",
    "        from the policy that is LONGER than 200 characters and best fits the answer, otherwise return an empty string, nothing else that differs from this. Make sure that your response\n",
    "        is only in a JSON format like this and DO NOT PROVIDE ANY ADDITIONAL TEXT: `{\"Answer\": \"Your answer\", \"Extract\": \"Extract from the policy\"}`, where \" Your Answer\" \n",
    "        represents your answer to the question, \"Extract from the policy\" is the best fit extract (make sure it is composed of whole sentences and also don't include any quatiotion marks). Here is the question: \"\"\" f\"{question_content['Text']}\"\n",
    "        print(len(context))\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4-1106-preview\",\n",
    "                    response_format={ \"type\": \"json_object\" },\n",
    "                    messages=[{\"role\":\"user\",\"content\": context}]\n",
    "                )\n",
    "        policy_response[questionid] = response.choices[0].message.content\n",
    "    return policy_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_by_policy_id(data, policy_id):\n",
    "    if policy_id in data.keys():\n",
    "        return data[policy_id]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_cached_questions(cached_questions, original_questions, question_embeddings):\n",
    "    missing_question_embeddings = {}\n",
    "    for key, value in original_questions.items():\n",
    "        if cached_questions:\n",
    "            if key not in cached_questions.keys():\n",
    "                missing_question_embeddings[key] = question_embeddings[key]\n",
    "        else:\n",
    "            return question_embeddings\n",
    "    return missing_question_embeddings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('search_questions.json', 'r', encoding='utf-8') as file:\n",
    "    questions_for_answering = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policies = {}\n",
    "\n",
    "for key, value in list(abstracts_json.items()):\n",
    "    test_policies[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_responses(responses, filepath):\n",
    "    with open(filepath, \"w\") as file:\n",
    "        json.dump(responses, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_txt(policy_id, response_content, file_path):\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(f\"PolicyID: {policy_id}\\n\")\n",
    "        file.write(f\"Response: {response_content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_trial(policy_id, response, file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        existing_data = json.load(file)\n",
    "    \n",
    "    existing_data[policy_id] = response\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(existing_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_policies(policies, questions_for_answering, cached_responses, cached_question_embeddings, question_embeddings_filepath, cached_response_filepath):\n",
    "    policies_responses = {}\n",
    "    questions_embeddings = {}\n",
    "    for questionid, question in questions_for_answering.items():\n",
    "        if questionid not in cached_question_embeddings:\n",
    "            question_content = {}\n",
    "            question_content[\"Embedding\"] = get_embedding(question)\n",
    "            question_content[\"Text\"] = question\n",
    "            questions_embeddings[questionid] = question_content\n",
    "            with open(question_embeddings_filepath, \"w\") as file:\n",
    "                json.dump(questions_embeddings, file, indent=4)\n",
    "        else:\n",
    "            questions_embeddings[questionid] = cached_question_embeddings[questionid]\n",
    "        \n",
    "    for policyid, policy in policies.items():\n",
    "        if policy: \n",
    "            cached_answers = get_response_by_policy_id(cached_responses, policyid)\n",
    "            missing_question_embeddings = get_non_cached_questions(cached_answers, questions_for_answering, questions_embeddings)\n",
    "            collection_name = \"whizz\"\n",
    "            collection_chroma = client.get_or_create_collection(name=collection_name)\n",
    "            response = policy_responses(policyid, policy, missing_question_embeddings, collection_chroma)\n",
    "            save_response_trial(policyid, response, cached_response_filepath)\n",
    "            policies_responses[policyid] = response\n",
    "    return policies_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_filepath = '' # filepath of json where answers are saved\n",
    "question_embeddings_filepath = '' # filepath for json where questions are embedded\n",
    "with open(cached_filepath, \"r\") as file:\n",
    "    cached_responses = json.load(file)\n",
    "with open(question_embeddings_filepath, \"r\") as file:\n",
    "    question_embeddings = json.load(file)\n",
    "responses = analyse_policies(abstracts_json, questions_all, cached_responses, question_embeddings, question_embeddings_filepath, cached_filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
